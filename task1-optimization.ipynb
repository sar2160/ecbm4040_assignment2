{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECBM E4040 Assignment 2 - Task 1: Optimization\n",
    "\n",
    "In this task, we introduce multiple SGD-based optimization methods. As you have learned from the last assignment, SGD is an efficient method to update parameters. However, to make SGD perform well, we need to find an appropriate learning rate and a good initial value. Otherwise, the network will get stuck if learning rate is small, or it will diverge if the learning rate is too large. In reality, since we have no prior knowledge about the training data, it is not trivial to find a good learning rate manually. Also, when the network becomes deeper, for each layer we may need to set a different learning rate, and that will again increase the development workload. Obviously, this is not a good direction. \n",
    "\n",
    "Another common problem is the lack of sufficient training data. This can make our training get stuck when using the naive SGD method. \n",
    "\n",
    "So, **how to set a good learning rate?** You are going to experiment with **SGD with momentum**, **RMSProp**, **Adam** and make comparisons.\n",
    "\n",
    "All of these optimizers are adaptive learning rate methods. Here is a useful link: http://ruder.io/optimizing-gradient-descent/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "% load_ext autoreload\n",
    "% autoreload 2\n",
    "\n",
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ecbm4040.cifar_utils import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR 10\n",
    "\n",
    "Here we use a small dataset with only 2500 samples to simulate \"lack-of-data\" situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start downloading data...\n",
      "Download complete.\n",
      "Training data shape:  (2000, 3072)\n",
      "Training labels shape:  (2000,)\n",
      "Validation data shape:  (500, 3072)\n",
      "Validation labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "# Load the raw CIFAR-10 data.\n",
    "X_train, y_train, X_test, y_test = load_data()\n",
    "X_val = X_train[:500,:]\n",
    "y_val = y_train[:500]\n",
    "X_train = X_train[500:2500,:]\n",
    "y_train = y_train[500:2500]\n",
    "\n",
    "mean_image = np.mean(X_train, axis=0).astype(np.float32)\n",
    "X_train = X_train.astype(np.float32) - mean_image\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "\n",
    "# We've vectorized the data for you. That is, we flatten the 32×32×3 images into 1×3072 Numpy arrays.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implement Optimizers\n",
    "\n",
    "Here we provide an MLP code snippet for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ecbm4040.neuralnets.mlp import MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original SGD (for comparison purpose only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 10\n",
      "epoch 1: valid acc = 0.146, new learning rate = 0.0095\n",
      "epoch 2: valid acc = 0.146, new learning rate = 0.009025\n",
      "epoch 3: valid acc = 0.154, new learning rate = 0.00857375\n",
      "epoch 4: valid acc = 0.152, new learning rate = 0.0081450625\n",
      "epoch 5: valid acc = 0.154, new learning rate = 0.007737809374999999\n",
      "epoch 6: valid acc = 0.164, new learning rate = 0.007350918906249998\n",
      "epoch 7: valid acc = 0.168, new learning rate = 0.006983372960937498\n",
      "epoch 8: valid acc = 0.17, new learning rate = 0.006634204312890623\n",
      "epoch 9: valid acc = 0.164, new learning rate = 0.006302494097246091\n",
      "epoch 10: valid acc = 0.162, new learning rate = 0.005987369392383786\n",
      "epoch 11: valid acc = 0.164, new learning rate = 0.005688000922764597\n",
      "epoch 12: valid acc = 0.164, new learning rate = 0.005403600876626367\n",
      "epoch 13: valid acc = 0.17, new learning rate = 0.005133420832795048\n",
      "epoch 14: valid acc = 0.168, new learning rate = 0.0048767497911552955\n",
      "epoch 15: valid acc = 0.168, new learning rate = 0.00463291230159753\n",
      "epoch 16: valid acc = 0.17, new learning rate = 0.0044012666865176535\n",
      "epoch 17: valid acc = 0.166, new learning rate = 0.004181203352191771\n",
      "epoch 18: valid acc = 0.168, new learning rate = 0.003972143184582182\n",
      "epoch 19: valid acc = 0.168, new learning rate = 0.0037735360253530726\n",
      "epoch 20: valid acc = 0.168, new learning rate = 0.0035848592240854188\n",
      "epoch 21: valid acc = 0.166, new learning rate = 0.0034056162628811476\n",
      "epoch 22: valid acc = 0.166, new learning rate = 0.0032353354497370902\n",
      "epoch 23: valid acc = 0.166, new learning rate = 0.0030735686772502355\n",
      "epoch 24: valid acc = 0.166, new learning rate = 0.0029198902433877237\n",
      "epoch 25: valid acc = 0.166, new learning rate = 0.0027738957312183374\n",
      "epoch 26: valid acc = 0.166, new learning rate = 0.0026352009446574203\n",
      "epoch 27: valid acc = 0.168, new learning rate = 0.002503440897424549\n",
      "epoch 28: valid acc = 0.168, new learning rate = 0.0023782688525533216\n",
      "epoch 29: valid acc = 0.168, new learning rate = 0.0022593554099256553\n",
      "epoch 30: valid acc = 0.168, new learning rate = 0.0021463876394293723\n"
     ]
    }
   ],
   "source": [
    "from ecbm4040.optimizers import SGDOptim\n",
    "\n",
    "model = MLP(input_dim=3072, hidden_dims=[100, 100], num_classes=10, weight_scale=1e-3, l2_reg=0.0)\n",
    "optimizer = SGDOptim()\n",
    "hist_sgd = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=30, batch_size=200, learning_rate=1e-2, learning_decay=0.95, \n",
    "                           verbose=False, record_interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD + Momentum\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span> Edit **SGDmomentumOptim** in __./ecbm4040/optimizers.py__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 10\n",
      "epoch 1: valid acc = 0.15, new learning rate = 0.0095\n",
      "epoch 2: valid acc = 0.164, new learning rate = 0.009025\n",
      "epoch 3: valid acc = 0.17, new learning rate = 0.00857375\n",
      "epoch 4: valid acc = 0.166, new learning rate = 0.0081450625\n",
      "epoch 5: valid acc = 0.17, new learning rate = 0.007737809374999999\n",
      "epoch 6: valid acc = 0.168, new learning rate = 0.007350918906249998\n",
      "epoch 7: valid acc = 0.174, new learning rate = 0.006983372960937498\n",
      "epoch 8: valid acc = 0.176, new learning rate = 0.006634204312890623\n",
      "epoch 9: valid acc = 0.192, new learning rate = 0.006302494097246091\n",
      "epoch 10: valid acc = 0.19, new learning rate = 0.005987369392383786\n",
      "epoch 11: valid acc = 0.192, new learning rate = 0.005688000922764597\n",
      "epoch 12: valid acc = 0.194, new learning rate = 0.005403600876626367\n",
      "epoch 13: valid acc = 0.192, new learning rate = 0.005133420832795048\n",
      "epoch 14: valid acc = 0.194, new learning rate = 0.0048767497911552955\n",
      "epoch 15: valid acc = 0.192, new learning rate = 0.00463291230159753\n",
      "epoch 16: valid acc = 0.186, new learning rate = 0.0044012666865176535\n",
      "epoch 17: valid acc = 0.186, new learning rate = 0.004181203352191771\n",
      "epoch 18: valid acc = 0.178, new learning rate = 0.003972143184582182\n",
      "epoch 19: valid acc = 0.18, new learning rate = 0.0037735360253530726\n",
      "epoch 20: valid acc = 0.18, new learning rate = 0.0035848592240854188\n",
      "epoch 21: valid acc = 0.184, new learning rate = 0.0034056162628811476\n",
      "epoch 22: valid acc = 0.182, new learning rate = 0.0032353354497370902\n",
      "epoch 23: valid acc = 0.184, new learning rate = 0.0030735686772502355\n",
      "epoch 24: valid acc = 0.186, new learning rate = 0.0029198902433877237\n",
      "epoch 25: valid acc = 0.186, new learning rate = 0.0027738957312183374\n",
      "epoch 26: valid acc = 0.188, new learning rate = 0.0026352009446574203\n",
      "epoch 27: valid acc = 0.188, new learning rate = 0.002503440897424549\n",
      "epoch 28: valid acc = 0.19, new learning rate = 0.0023782688525533216\n",
      "epoch 29: valid acc = 0.186, new learning rate = 0.0022593554099256553\n",
      "epoch 30: valid acc = 0.18, new learning rate = 0.0021463876394293723\n"
     ]
    }
   ],
   "source": [
    "from ecbm4040.optimizers import SGDmomentumOptim\n",
    "\n",
    "model = MLP(input_dim=3072, hidden_dims=[100, 100], num_classes=10, l2_reg=0.0, weight_scale=1e-3)\n",
    "optimizer = SGDmomentumOptim(model, momentum=0.8)\n",
    "hist_sgd_momentum = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                                         num_epoch=30, batch_size=200, learning_rate=1e-2, \n",
    "                                         learning_decay=0.95, verbose=False, record_interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span> Edit **RMSpropOptim** in **./ecbm4040/optimizers.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 10\n",
      "epoch 1: valid acc = 0.106, new learning rate = 0.00095\n",
      "epoch 2: valid acc = 0.1, new learning rate = 0.0009025\n",
      "epoch 3: valid acc = 0.1, new learning rate = 0.000857375\n",
      "epoch 4: valid acc = 0.1, new learning rate = 0.0008145062499999999\n",
      "epoch 5: valid acc = 0.1, new learning rate = 0.0007737809374999998\n",
      "epoch 6: valid acc = 0.076, new learning rate = 0.0007350918906249997\n",
      "epoch 7: valid acc = 0.098, new learning rate = 0.0006983372960937497\n",
      "epoch 8: valid acc = 0.122, new learning rate = 0.0006634204312890621\n",
      "epoch 9: valid acc = 0.126, new learning rate = 0.000630249409724609\n",
      "epoch 10: valid acc = 0.17, new learning rate = 0.0005987369392383785\n",
      "epoch 11: valid acc = 0.164, new learning rate = 0.0005688000922764595\n",
      "epoch 12: valid acc = 0.158, new learning rate = 0.0005403600876626365\n",
      "epoch 13: valid acc = 0.16, new learning rate = 0.0005133420832795047\n",
      "epoch 14: valid acc = 0.174, new learning rate = 0.00048767497911552944\n",
      "epoch 15: valid acc = 0.162, new learning rate = 0.00046329123015975297\n",
      "epoch 16: valid acc = 0.182, new learning rate = 0.0004401266686517653\n",
      "epoch 17: valid acc = 0.188, new learning rate = 0.00041812033521917703\n",
      "epoch 18: valid acc = 0.234, new learning rate = 0.00039721431845821814\n",
      "epoch 19: valid acc = 0.254, new learning rate = 0.0003773536025353072\n",
      "epoch 20: valid acc = 0.242, new learning rate = 0.0003584859224085418\n",
      "epoch 21: valid acc = 0.234, new learning rate = 0.0003405616262881147\n",
      "epoch 22: valid acc = 0.31, new learning rate = 0.00032353354497370894\n",
      "epoch 23: valid acc = 0.298, new learning rate = 0.00030735686772502346\n",
      "epoch 24: valid acc = 0.306, new learning rate = 0.00029198902433877225\n",
      "epoch 25: valid acc = 0.33, new learning rate = 0.00027738957312183364\n",
      "epoch 26: valid acc = 0.298, new learning rate = 0.0002635200944657419\n",
      "epoch 27: valid acc = 0.332, new learning rate = 0.0002503440897424548\n",
      "epoch 28: valid acc = 0.338, new learning rate = 0.00023782688525533205\n",
      "epoch 29: valid acc = 0.312, new learning rate = 0.00022593554099256544\n",
      "epoch 30: valid acc = 0.338, new learning rate = 0.00021463876394293716\n"
     ]
    }
   ],
   "source": [
    "from ecbm4040.optimizers import RMSpropOptim\n",
    "\n",
    "model = MLP(input_dim=3072, hidden_dims=[100, 100], num_classes=10, l2_reg=0.0, weight_scale=1e-3)\n",
    "optimizer = RMSpropOptim(model)\n",
    "hist_rmsprop = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                               num_epoch=30, batch_size=200, learning_rate=1e-3, \n",
    "                               learning_decay=0.95, verbose=False, record_interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span> Edit **AdamOptim** in **./ecbm4040/optimizers.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 10\n",
      "epoch 1: valid acc = 0.164, new learning rate = 0.00095\n",
      "epoch 2: valid acc = 0.16, new learning rate = 0.0009025\n",
      "epoch 3: valid acc = 0.17, new learning rate = 0.000857375\n",
      "epoch 4: valid acc = 0.194, new learning rate = 0.0008145062499999999\n",
      "epoch 5: valid acc = 0.246, new learning rate = 0.0007737809374999998\n",
      "epoch 6: valid acc = 0.26, new learning rate = 0.0007350918906249997\n",
      "epoch 7: valid acc = 0.284, new learning rate = 0.0006983372960937497\n",
      "epoch 8: valid acc = 0.3, new learning rate = 0.0006634204312890621\n",
      "epoch 9: valid acc = 0.302, new learning rate = 0.000630249409724609\n",
      "epoch 10: valid acc = 0.324, new learning rate = 0.0005987369392383785\n",
      "epoch 11: valid acc = 0.326, new learning rate = 0.0005688000922764595\n",
      "epoch 12: valid acc = 0.318, new learning rate = 0.0005403600876626365\n",
      "epoch 13: valid acc = 0.322, new learning rate = 0.0005133420832795047\n",
      "epoch 14: valid acc = 0.312, new learning rate = 0.00048767497911552944\n",
      "epoch 15: valid acc = 0.348, new learning rate = 0.00046329123015975297\n",
      "epoch 16: valid acc = 0.372, new learning rate = 0.0004401266686517653\n",
      "epoch 17: valid acc = 0.366, new learning rate = 0.00041812033521917703\n",
      "epoch 18: valid acc = 0.374, new learning rate = 0.00039721431845821814\n",
      "epoch 19: valid acc = 0.348, new learning rate = 0.0003773536025353072\n",
      "epoch 20: valid acc = 0.33, new learning rate = 0.0003584859224085418\n",
      "epoch 21: valid acc = 0.342, new learning rate = 0.0003405616262881147\n",
      "epoch 22: valid acc = 0.356, new learning rate = 0.00032353354497370894\n",
      "epoch 23: valid acc = 0.37, new learning rate = 0.00030735686772502346\n",
      "epoch 24: valid acc = 0.346, new learning rate = 0.00029198902433877225\n",
      "epoch 25: valid acc = 0.338, new learning rate = 0.00027738957312183364\n",
      "epoch 26: valid acc = 0.338, new learning rate = 0.0002635200944657419\n",
      "epoch 27: valid acc = 0.368, new learning rate = 0.0002503440897424548\n",
      "epoch 28: valid acc = 0.364, new learning rate = 0.00023782688525533205\n",
      "epoch 29: valid acc = 0.358, new learning rate = 0.00022593554099256544\n",
      "epoch 30: valid acc = 0.348, new learning rate = 0.00021463876394293716\n"
     ]
    }
   ],
   "source": [
    "from ecbm4040.optimizers import AdamOptim\n",
    "\n",
    "model = MLP(input_dim=3072, hidden_dims=[100, 100], num_classes=10, l2_reg=0.0, weight_scale=1e-3)\n",
    "optimizer = AdamOptim(model)\n",
    "hist_adam = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                            num_epoch=30, batch_size=200, learning_rate=1e-3, \n",
    "                            learning_decay=0.95, verbose=False, record_interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Comparison\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span> Run the following cells, which plot the loss curves of different optimizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_hist_sgd, train_acc_hist_sgd, val_acc_hist_sgd = hist_sgd\n",
    "loss_hist_momentum, train_acc_hist_momentum, val_acc_hist_momentum = hist_sgd_momentum\n",
    "loss_hist_rmsprop, train_acc_hist_rmsprop, val_acc_hist_rmsprop = hist_rmsprop\n",
    "loss_hist_adam, train_acc_hist_adam, val_acc_hist_adam = hist_adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot training error curve of optimizers\n",
    "plt.plot(loss_hist_sgd, label=\"sgd\")\n",
    "plt.plot(loss_hist_momentum, label=\"momentum\")\n",
    "plt.plot(loss_hist_rmsprop, label=\"rmsprop\")\n",
    "plt.plot(loss_hist_adam, label=\"adam\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot training accuracy curve of optimizers\n",
    "plt.plot(train_acc_hist_sgd, label=\"sgd\")\n",
    "plt.plot(train_acc_hist_momentum, label=\"momentum\")\n",
    "plt.plot(train_acc_hist_rmsprop, label=\"rmsprop\")\n",
    "plt.plot(train_acc_hist_adam, label=\"adam\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot validation accuracy curve of optimizers\n",
    "plt.plot(val_acc_hist_sgd, label=\"sgd\")\n",
    "plt.plot(val_acc_hist_momentum, label=\"momentum\")\n",
    "plt.plot(val_acc_hist_rmsprop, label=\"rmsprop\")\n",
    "plt.plot(val_acc_hist_adam, label=\"adam\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"color:red\">__TODO:__</span> Describe your results, and discuss your understandings of these optimizers, such as their advantages/disadvantages and when to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: **[fill in here]**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
